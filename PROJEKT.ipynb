{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVk5119xIjePfcZQ92y+/P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pankotaiimre2001/xdlol/blob/master/PROJEKT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euI4aXRc351j",
        "outputId": "0a11c02d-b9aa-4ad9-a83a-8beedd023629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huspacy in /usr/local/lib/python3.11/dist-packages (0.12.1)\n",
            "Requirement already satisfied: packaging<22.0,>=21.3 in /usr/local/lib/python3.11/dist-packages (from huspacy) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from packaging<22.0,>=21.3->huspacy) (3.2.3)\n",
            "Magyar spaCy modell (hu_core_news_lg) sikeresen betöltve.\n",
            "\n",
            "--- 1. HTML Tartalom Letöltése: https://index.hu/kulfold/2025/05/22/zsolyomi-zsolt-gyilkossag-homoszexualis-szereto-furdokad-biztonsagi-ov-miami-florida-eszbontok/ ---\n",
            "Az oldal sikeresen letöltve (hossz: 101133 karakter).\n",
            "\n",
            "--- 2. Fő Cikkszöveg Kinyerése ---\n",
            "Figyelem: Nem találtuk a szokásos cikk konténert. Próbálkozás az összes <p> taggel.\n",
            "Kinyert nyers szöveg hossza: 7065 karakter.\n",
            "Kinyert szöveg eleje (max 500 karakter):\n",
            "A következő oldal tartalma a kiskorúakra káros lehet. Ha korlátozná a korhatáros tartalmak elérését a gépén, használjon szűrőprogramot ! Az oldal tartalma az Mttv. által rögzített besorolás szerinti V. vagy VI. kategóriába tartozik. Kövesse az Indexet Facebookon is! Egy csendes Pest vármegyei faluból indult, és Miamiban kötött ki, ahol két idős férfi életét oltotta ki hidegvérrel. Zsólyomi Zsolt története a magyar kriminálisok sötét fejezetei közé tartozik. A gyilkos neve sokaknak lehet ismerős ...\n",
            "\n",
            "--- 3. Szöveg Tisztítása és Előfeldolgozása ---\n",
            "Feldolgozott szöveg (első 500 karakter):\n",
            "a következő oldal tartalom a kiskorúa káros lesz ha korlátoz a korhatáros tartalom elérés a gép használ szűrőprogram az oldal tartalom az mttv által rögzített besorolás szerinti v. vagy vi kategória tartozik követ az index faceboo is egy csendes pest vármegyei falu indul és miami köt ki ahol két idős férfi élet olt ki hidegvér zsólyomi zsolt történet a magyar kriminális sötét fejezet közé tartozik a gyilkos név sok lesz ismerős az észbontó című magyar tévéműsor ahol különböző némi általános műve...\n",
            "\n",
            "--- 4. Vektor Reprezentáció Generálása ---\n",
            "A 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2' beágyazó modell már be van töltve.\n",
            "Beágyazás generálása a feldolgozott szöveghez...\n",
            "1 darabra bontva a hosszú szöveg.\n",
            "Generált beágyazás alakja: (384,)\n",
            "Beágyazás első 5 értéke: [-0.0039648   0.09754702 -0.12796581 -0.15939724  0.34331256]\n",
            "\n",
            "--- Feldolgozás vége ---\n",
            "A feldolgozott szöveg elmentve ide: feldolgozott_szoveg.txt\n",
            "A nyers szöveg elmentve ide: nyers_szoveg.txt\n"
          ]
        }
      ],
      "source": [
        "# === 0. KÖNYVTÁRAK IMPORTÁLÁSA ÉS BEÁLLÍTÁSOK ===\n",
        "import requests                     # HTTP kérésekhez (weboldal letöltése)\n",
        "from bs4 import BeautifulSoup       # HTML parse-oláshoz\n",
        "import pandas as pd                 # Adatkezeléshez (DataFrame)\n",
        "import re                           # Reguláris kifejezésekhez\n",
        "import unicodedata                  # Unicode normalizáláshoz\n",
        "from functools import lru_cache\n",
        "!pip install huspacy\n",
        "import huspacy\n",
        "#huspacy.download()\n",
        "\n",
        "import spacy                        # NLP feladatokhoz (lemmatizálás)\n",
        "from sentence_transformers import SentenceTransformer # Beágyazások generálásához\n",
        "import torch                        # PyTorch (gyakran a transformers könyvtárak függősége)\n",
        "\n",
        "# Figyelmeztetések kikapcsolása (opcionális, ha zavaróak a könyvtárak figyelmeztetései)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Magyar spaCy modell betöltése\n",
        "# Győződj meg róla, hogy telepítetted Colab-ban: !python -m spacy download hu_core_news_lg\n",
        "try:\n",
        "    nlp_hu = huspacy.load()\n",
        "    print(\"Magyar spaCy modell (hu_core_news_lg) sikeresen betöltve.\")\n",
        "except OSError:\n",
        "    print(\"HIBA: A magyar spaCy modell (hu_core_news_lg) nincs telepítve vagy nem tölthető be.\")\n",
        "    print(\"Google Colab-ban futtasd: !python -m spacy download hu_core_news_lg\")\n",
        "    print(\"Majd indítsd újra a futási környezetet (Runtime -> Restart session).\")\n",
        "    nlp_hu = None\n",
        "\n",
        "# A cél URL\n",
        "TARGET_URL = \"https://index.hu/kulfold/2025/05/22/zsolyomi-zsolt-gyilkossag-homoszexualis-szereto-furdokad-biztonsagi-ov-miami-florida-eszbontok/\"\n",
        "\n",
        "@lru_cache(maxsize=1024)\n",
        "def lemmatize_text_cached(text):\n",
        "    return lemmatize_text_with_spacy(text)\n",
        "\n",
        "# === 1. HTML TARTALOM LETÖLTÉSE ===\n",
        "print(f\"\\n--- 1. HTML Tartalom Letöltése: {TARGET_URL} ---\")\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "html_content_article = None\n",
        "try:\n",
        "    response = requests.get(TARGET_URL, headers=headers, timeout=15) # Timeout hozzáadása\n",
        "    response.raise_for_status()\n",
        "    html_content_article = response.text\n",
        "    print(f\"Az oldal sikeresen letöltve (hossz: {len(html_content_article)} karakter).\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Hiba történt az oldal letöltése közben: {e}\")\n",
        "\n",
        "# === 2. FŐ CIKKSZÖVEG KINYERÉSE A HTML-BŐL ===\n",
        "# Ez a rész erősen függ az adott weboldal HTML szerkezetétől.\n",
        "# Javasolt a böngésző Fejlesztői Eszközeivel (F12) megvizsgálni a céloldalt\n",
        "# és azonosítani a fő cikkszöveget tartalmazó HTML elemeket (tagek, class-ok, id-k).\n",
        "print(\"\\n--- 2. Fő Cikkszöveg Kinyerése ---\")\n",
        "extracted_article_text_parts = []\n",
        "if html_content_article:\n",
        "    soup = BeautifulSoup(html_content_article, \"lxml\")\n",
        "\n",
        "    # Specifikus próbálkozás a FrontiersIn oldalakhoz (ezeket finomítsd a böngészős vizsgálat alapján!)\n",
        "\n",
        "    # Absztrakt\n",
        "    abstract_section = soup.find('div', class_='JournalAbstract')\n",
        "    if abstract_section:\n",
        "        extracted_article_text_parts.append(\"Absztrakt: \" + abstract_section.get_text(separator=\" \", strip=True))\n",
        "\n",
        "    # Kulcsszavak\n",
        "    keywords_section = soup.find('div', class_='Keywords')\n",
        "    if keywords_section:\n",
        "        extracted_article_text_parts.append(\"Kulcsszavak: \" + keywords_section.get_text(separator=\" \", strip=True))\n",
        "\n",
        "    # Fő cikktörzs (a `div[role=\"article\"]` tűnik a legrelevánsabb konténernek)\n",
        "    article_body = soup.find('div', class_='article-content') or soup.find('article')\n",
        "    if article_body:\n",
        "        paragraphs = article_body.find_all('p')\n",
        "        print(f\"Talált cikktartalom {len(paragraphs)} bekezdéssel.\")\n",
        "    else:\n",
        "        print(\"Figyelem: Nem találtuk a szokásos cikk konténert. Próbálkozás az összes <p> taggel.\")\n",
        "        paragraphs = soup.find_all('p')\n",
        "\n",
        "    relevant_paragraphs = [p.get_text(separator=\" \", strip=True) for p in paragraphs]\n",
        "    extracted_article_text_parts.extend(relevant_paragraphs)\n",
        "\n",
        "\n",
        "    # Üres stringek eltávolítása és összefűzés\n",
        "    full_extracted_text = \" \".join(filter(None, extracted_article_text_parts))\n",
        "\n",
        "    if not full_extracted_text.strip() and html_content_article:\n",
        "        print(\"Figyelem: A célzott kinyerés nem adott eredményt, próbálkozás a teljes body szövegével.\")\n",
        "        full_extracted_text = soup.body.get_text(separator=\" \", strip=True) if soup.body else \"Nem sikerült szöveget kinyerni.\"\n",
        "else:\n",
        "    full_extracted_text = \"Az oldal HTML tartalma nem érhető el.\"\n",
        "\n",
        "print(f\"Kinyert nyers szöveg hossza: {len(full_extracted_text)} karakter.\")\n",
        "print(\"Kinyert szöveg eleje (max 500 karakter):\")\n",
        "print(full_extracted_text[:500] + \"...\" if len(full_extracted_text) > 500 else full_extracted_text)\n",
        "\n",
        "# === 3. SZÖVEG TISZTÍTÁSA ÉS ELŐFELDOLGOZÁSA ===\n",
        "print(\"\\n--- 3. Szöveg Tisztítása és Előfeldolgozása ---\")\n",
        "\n",
        "# Adat DataFrame-be helyezése\n",
        "df_article_processed = pd.DataFrame({'id': [TARGET_URL], 'raw_extracted_text': [full_extracted_text]})\n",
        "\n",
        "# --- Segédfüggvények a tisztításhoz ---\n",
        "def clean_special_chars_and_normalize(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text)\n",
        "    text = unicodedata.normalize(\"NFKC\", text) # Unicode normalizálás\n",
        "    # HTML entitások dekódolása (ha a BeautifulSoup nem kezelte volna mindet)\n",
        "    # from html import unescape\n",
        "    # text = unescape(text) # Ezt óvatosan, lehet, hogy nem szükséges\n",
        "    text = re.sub(r\"\\[\\d+\\]\", \"\", text) # [1], [23] stílusú hivatkozások eltávolítása\n",
        "    text = re.sub(r\"\\[\\d+-\\d+\\]\", \"\", text) # [1-5] stílusú hivatkozások eltávolítása\n",
        "    text = re.sub(r\"http\\S+\", \"\", text) # URL-ek eltávolítása\n",
        "    # Csak betűk, számok, magyar ékezetes karakterek és alapvető írásjelek megtartása.\n",
        "    # Ez a regex szigorúbb lehet, pl. a zárójeleket, idézőjeleket is megtarthatod, ha fontosak.\n",
        "    text = re.sub(r\"[^a-zA-Z0-9áéíóöőúüűÁÉÍÓÖŐÚÜŰ\\s.,!?:;\\-]\", \" \", text) # Figyelj a kötőjelre a végén\n",
        "    text = \" \".join(text.split()) # Extra szóközök eltávolítása\n",
        "    return text.strip()\n",
        "\n",
        "def lemmatize_text_with_spacy(text_to_lemmatize):\n",
        "    if pd.isna(text_to_lemmatize) or not nlp_hu:\n",
        "        return str(text_to_lemmatize) if not pd.isna(text_to_lemmatize) else \"\"\n",
        "\n",
        "    doc = nlp_hu(str(text_to_lemmatize))\n",
        "    # Csak bizonyos PoS tageket tartunk meg, pl. főnevek, melléknevek, igék (opcionális)\n",
        "    # allowed_pos = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
        "    # lemmas = [token.lemma_ for token in doc if token.pos_ in allowed_pos and not token.is_stop]\n",
        "    lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space] # Lemmatizálunk, de a központozást és szóközöket kihagyjuk\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "def split_text_to_chunks(text, max_chunk_tokens=500, model=None):\n",
        "    \"\"\"\n",
        "    Szöveg darabolása kb. max_chunk_tokens hosszúságú darabokra.\n",
        "    A darabolás mondatok szerint történik.\n",
        "    \"\"\"\n",
        "    if not model:\n",
        "        raise ValueError(\"A 'model' paraméter megadása kötelező, hogy tudjuk hány token egy-egy mondat.\")\n",
        "\n",
        "    sentences = text.split('.')  # Egyszerű mondatbontás\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        candidate = (current_chunk + \" \" + sentence).strip()\n",
        "        n_tokens = len(model.tokenize(candidate))\n",
        "\n",
        "        if n_tokens <= max_chunk_tokens:\n",
        "            current_chunk = candidate\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence.strip()\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def get_average_embedding(text, model, max_chunk_tokens=500):\n",
        "    chunks = split_text_to_chunks(text, max_chunk_tokens=max_chunk_tokens, model=model)\n",
        "    print(f\"{len(chunks)} darabra bontva a hosszú szöveg.\")\n",
        "\n",
        "    embeddings = model.encode(chunks)\n",
        "\n",
        "    # Átlagolás (mean pooling)\n",
        "    average_embedding = sum(torch.tensor(e) for e in embeddings) / len(embeddings)\n",
        "    return average_embedding.numpy()\n",
        "\n",
        "# --- Tisztítási lépések alkalmazása ---\n",
        "df_article_processed['cleaned_text'] = df_article_processed['raw_extracted_text'].apply(clean_special_chars_and_normalize)\n",
        "df_article_processed['normalized_text'] = df_article_processed['cleaned_text'].str.lower()\n",
        "\n",
        "# Lemmatizálás (opcionális - kapcsold ki/be a következő sort, ha kell/nem kell)\n",
        "# A SentenceTransformer modellek általában jobban működnek lemmatizálás nélkül.\n",
        "# Ha használod, a 'final_processed_text' legyen a 'lemmatized_text'.\n",
        "def lemmatize_texts_batch(texts, nlp_model):\n",
        "    docs = nlp_model.pipe(texts, batch_size=16, disable=[\"ner\"])  # Gyorsabb NER nélkül\n",
        "    lemmatized_texts = []\n",
        "    for doc in docs:\n",
        "        lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
        "        lemmatized_texts.append(\" \".join(lemmas))\n",
        "    return lemmatized_texts\n",
        "\n",
        "df_article_processed['lemmatized_text'] = lemmatize_texts_batch(df_article_processed['normalized_text'].tolist(), nlp_hu)\n",
        "df_article_processed['final_processed_text'] = df_article_processed['lemmatized_text']\n",
        "\n",
        "#df_article_processed['final_processed_text'] = df_article_processed['normalized_text'] # Lemmatizálás nélkül\n",
        "\n",
        "print(\"Feldolgozott szöveg (első 500 karakter):\")\n",
        "final_text_sample = df_article_processed['final_processed_text'].iloc[0]\n",
        "print(final_text_sample[:500] + \"...\" if len(final_text_sample) > 500 else final_text_sample)\n",
        "# display(df_article_processed.head()) # DataFrame megjelenítése\n",
        "\n",
        "# === 4. VEKTOR REPREZENTÁCIÓ (BEÁGYAZÁSOK) GENERÁLÁSA ===\n",
        "print(\"\\n--- 4. Vektor Reprezentáció Generálása ---\")\n",
        "\n",
        "# Beágyazó modell betöltése (ha még nem történt meg)\n",
        "try:\n",
        "    sbert_model # Ellenőrizzük, létezik-e már a változó\n",
        "    print(f\"A '{sbert_model_name}' beágyazó modell már be van töltve.\")\n",
        "except NameError:\n",
        "    sbert_model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
        "    # Alternatívák: 'sentence-transformers/distiluse-base-multilingual-cased-v1'\n",
        "    print(f\"Beágyazó modell betöltése: {sbert_model_name}\")\n",
        "    sbert_model = SentenceTransformer(sbert_model_name)\n",
        "\n",
        "text_for_embedding = df_article_processed['final_processed_text'].iloc[0]\n",
        "article_embedding_vector = None\n",
        "\n",
        "if text_for_embedding and text_for_embedding.strip():\n",
        "    print(f\"Beágyazás generálása a feldolgozott szöveghez...\")\n",
        "    # A .encode() várhat stringet vagy stringek listáját.\n",
        "    article_embedding_vector = get_average_embedding(text_for_embedding, sbert_model)\n",
        "\n",
        "    print(\"Generált beágyazás alakja:\", article_embedding_vector.shape)\n",
        "    print(\"Beágyazás első 5 értéke:\", article_embedding_vector[:5])\n",
        "\n",
        "    # A beágyazást hozzáadhatod a DataFrame-hez vagy elmentheted\n",
        "    # df_article_processed['embedding'] = [article_embedding_vector]\n",
        "    # np.save('single_article_embedding.npy', article_embedding_vector)\n",
        "else:\n",
        "    print(\"Nincs feldolgozott szöveg a beágyazáshoz.\")\n",
        "\n",
        "print(\"\\n--- Feldolgozás vége ---\")\n",
        "\n",
        "output_filename = \"feldolgozott_szoveg.txt\"\n",
        "\n",
        "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(df_article_processed['final_processed_text'].iloc[0])\n",
        "\n",
        "print(f\"A feldolgozott szöveg elmentve ide: {output_filename}\")\n",
        "\n",
        "output_filename = \"nyers_szoveg.txt\"\n",
        "\n",
        "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(df_article_processed['raw_extracted_text'].iloc[0])\n",
        "\n",
        "print(f\"A nyers szöveg elmentve ide: {output_filename}\")\n"
      ]
    }
  ]
}